================================================================================
                    AI EMAIL HELPER APPLICATION - REPORT
                         Module 4: Model Evaluation
================================================================================

1. EXECUTIVE SUMMARY
--------------------------------------------------------------------------------

This report documents the development and evaluation of an AI-powered email 
editing application built using Streamlit and Azure OpenAI. The application 
enables users to transform emails through three actions: shorten, lengthen, 
and tone transformation (professional, friendly, sympathetic).

Key Statistics:
- Models Evaluated: gpt-4o-mini and gpt-4.1
- Total Dataset Size: 254 emails (original + synthetic)
  - Shorten: 72 emails (52 original + 20 synthetic)
  - Lengthen: 65 emails (50 original + 15 synthetic)
  - Tone: 117 emails (102 original + 15 synthetic)
- Evaluation Metrics: 6 (Faithfulness, Completeness, Conciseness, 
  Grammar & Clarity, Tone Consistency, URL Preservation)
- Synthetic Data Generated: 50 records with edge cases

The evaluation revealed that gpt-4o-mini performed as the best generator with 
an average score of 9.65/10 across all 6 metrics. The strongest metric was 
Grammar & Clarity. Both models performed well on URL preservation when 
shortening emails. The overall evaluation average was 9.62/10, indicating 
high-quality transformations from both models.


2. MODEL OVERVIEW
--------------------------------------------------------------------------------

Generator Models:
- gpt-4o-mini: Best performing model with avg 9.65/10, faster response times
- gpt-4.1: Capable model with strong reasoning for complex transformations

Judge Model:
- gpt-4.1 (primary): Used as the default judge for evaluation scoring
- gpt-4o-mini (optional): Available for judge model comparison


3. METHODS AND PERFORMANCE METRICS
--------------------------------------------------------------------------------

3.1 Development Process
-----------------------

Phase 1: Environment Setup
- Cloned starter repository with basic Streamlit structure
- Configured Python 3.12 virtual environment
- Installed dependencies: streamlit, openai, pyyaml, pandas, plotly
- Set up Azure OpenAI API credentials in .env file

Phase 2: UI Development
- Created two-page Streamlit application architecture
- Page 1: Email editing with model selection and generation
- Page 2: Evaluation dashboard with batch processing and charts
- Added conditional tone selector (appears only for tone dataset)
- Built email content display with editable text area
- Added model selector for both generator and judge models

Phase 3: Backend Integration
- Created GenerateEmail class for API interactions
- Implemented prompt loading from prompts.yaml
- Built generate() method for email transformations
- Added partial text transformation feature (transform selected portion only)

Phase 4: Evaluation System
- Implemented judge() method with 6 evaluation metrics
- Created separate evaluation dashboard page (pages/evaluation.py)
- Added batch evaluation with progress tracking
- Built visualization charts: Bar, Radar, Heatmap
- Added model response preview with toggle for generated emails
- Added CSV export functionality

3.2 Evaluation Metrics
----------------------

1. Faithfulness (0-10): Does the edited email preserve the original meaning 
   and intent without adding false information?

2. Completeness (0-10): Are all key points from the original retained?

3. Conciseness (0-10): Is the email efficiently written without unnecessary 
   words or redundancy?

4. Grammar & Clarity (0-10): Is the email grammatically correct and clearly 
   written? (Strongest metric in evaluation)

5. Tone & Style Consistency (0-10): Does the email maintain a consistent 
   tone throughout?

6. URL Preservation (0-10): Are all original URLs, hyperlinks, and references 
   accurately retained?

3.3 Prompt Engineering
----------------------

Initial Challenge: Early evaluation results showed unrealistically high scores 
(9-10 across all metrics), indicating potential bias in the judge prompts.

Solution: Used ChatGPT to refine judge prompts with:
- Stricter evaluation criteria
- Explicit instructions to "deduct points for ANY deviation"
- Requirement for specific examples in explanations
- JSON output format for consistent parsing

Result: After refining prompts, scores became more realistic with a range of 
6-9 instead of all 9-10, helping identify actual weaknesses like faithfulness 
issues in certain transformations.

Example Prompt Refinement (Faithfulness):
- Before: "Rate how faithful the edit is"
- After: "You are a STRICT evaluator. Deduct points for ANY deviation from 
  the original meaning. A score of 10 means PERFECT preservation with zero 
  alterations to facts or intent."

3.4 Evaluation Results
----------------------

Generator Model Comparison (Shorten Dataset):
| Metric              | gpt-4o-mini | gpt-4.1 |
|---------------------|-------------|---------|
| Faithfulness        | 9.6         | 9.4     |
| Completeness        | 9.7         | 9.5     |
| Conciseness         | 9.6         | 9.4     |
| Grammar & Clarity   | 9.8         | 9.6     |
| Tone Consistency    | 9.6         | 9.5     |
| URL Preservation    | 9.6         | 9.5     |
| Overall Average     | 9.65        | 9.48    |

Response Time Comparison:
| Model        | Avg Time (sec) |
|--------------|----------------|
| gpt-4o-mini  | 1.47           |
| gpt-4.1      | 1.20           |

Key Findings:
- Best Generator: gpt-4o-mini (Avg: 9.65/10)
- Strongest Metric: Grammar & Clarity
- Overall Average: 9.62/10
- Both models performed well on URL preservation
- gpt-4.1 was faster but gpt-4o-mini had better faithfulness


4. SYNTHETIC DATA GENERATION
--------------------------------------------------------------------------------

4.1 Rationale
-------------

Upon examining the original datasets, several gaps were identified:
- No emails with embedded URLs or hyperlinks
- Limited representation of very short or very long emails
- No HR/IT ServiceNow-related content
- Lack of emails with vague or unclear requests

4.2 Data Generation Breakdown
-----------------------------

Shorten Dataset (20 synthetic):
- 5 extremely verbose business emails (500+ words)
- 5 emails with multiple embedded URLs
- 5 IT/HR ServiceNow-related emails
- 5 meeting summary emails

Lengthen Dataset (15 synthetic):
- 5 extremely short emails (1-2 sentences, no context)
- 5 bullet-point only emails
- 5 incomplete request emails

Tone Dataset (15 synthetic):
- 5 emails with mixed/inconsistent tone
- 5 formal complaint emails
- 5 casual emails requiring professionalization

4.3 Synthetic Data Integration
------------------------------

All synthetic data was merged into original datasets with a "type" field:
- "type": "original" - for pre-existing emails
- "type": "synthetic" - for generated edge cases

This allows filtering during evaluation to compare model performance on 
standard vs edge case scenarios.


5. USE CASES AND APPLICATIONS
--------------------------------------------------------------------------------

5.1 Email Shortening
- Use Case: Condensing long status updates or meeting notes
- Practical Score: 9.6/10 for standard emails
- Both models performed well with preserving links when shortening

5.2 Email Lengthening  
- Use Case: Expanding brief notes into professional communications
- Practical Score: 9.5/10 for standard emails

5.3 Tone Transformation
- Use Case: Adjusting customer service responses to match brand voice
- Options: Professional, Friendly, Sympathetic
- Practical Score: 9.5/10 across all tone options

5.4 Partial Text Transformation (Optional Feature)
--------------------------------------------------

This feature allows users to select and transform only a specific portion 
of an email while keeping the rest unchanged. Useful for editing specific 
paragraphs without affecting the entire message.

Test Case:
- Dataset: Shorten
- Selected Portion: Middle paragraph
- Original Selected Text Word Count: 42 words
- Transformed Text Word Count: 34 words

Evaluation Checklist:
[X] Transformation Applied: Did the selected text get properly transformed?
[X] Preservation: Did the surrounding text remain exactly unchanged?
[X] Natural Flow: Does the edited portion blend smoothly with the rest?
[X] No Artifacts: Are there any formatting issues at the edit boundaries?

Result: Pass

Notes: Although the difference is not much (42 â†’ 34 words), the transformation 
tightened the paragraph by condensing phrases, thereby retaining the meaning 
and improving the flow of the original content of the email.


6. LIMITATIONS AND RISKS
--------------------------------------------------------------------------------

6.1 Model Limitations
- Context Loss: Very short emails lack context for meaningful lengthening
- Tone Blending: Complex emotional emails may not transform cleanly
- Subject Line Insertion: Models occasionally insert "Subject:" into content

6.2 Evaluation Limitations
- Self-Judging Bias: Using LLMs to judge LLM output has inherent limitations
- Metric Overlap: Some metrics (Conciseness, Grammar) may correlate strongly
- High Scores: Even after prompt refinement, scores tend toward the higher end

6.3 Observations
- gpt-4o-mini performed slightly better overall but was slower (1.47s vs 1.2s)
- gpt-4o-mini had better faithfulness scores than gpt-4.1
- gpt-4.1 was faster but scored slightly lower on quality metrics
- Both models handled URL preservation well in shortening tasks
- Grammar & Clarity was consistently the strongest metric
- Synthetic edge cases (verbose, URL-heavy) were handled effectively


7. CONCLUSIONS AND NEXT STEPS
--------------------------------------------------------------------------------

7.1 Key Takeaways
-----------------

1. Model Selection & Complementary Approach:
   Based on evaluation results, gpt-4o-mini demonstrated stronger performance 
   in text generation tasks (email transformation), while gpt-4.1's reasoning 
   capabilities made it suitable as the evaluation judge model. This suggests 
   a complementary approach: using gpt-4o-mini for generation and gpt-4.1 for 
   quality assessment.
   
   - gpt-4o-mini for Generation:
     * Highest overall score (9.65/10)
     * Better faithfulness scores
     * Strong performance across all metrics
     * Note: Slightly slower (1.47s vs 1.2s)
   
   - gpt-4.1 for Evaluation/Reasoning:
     * Used as judge model for quality assessment
     * Faster response times (1.2s)
     * Strong reasoning capabilities for analysis tasks

2. Framework Value: The evaluation framework with 6 metrics provides 
   comprehensive quality assessment for email transformations.

3. Prompt Engineering Matters: Refined judge prompts reduced score inflation, 
   providing more accurate evaluation and helping identify actual weaknesses.

4. Edge Cases Handled Well: Both models performed effectively on synthetic 
   data including verbose emails, URL-heavy content, and IT/HR scenarios.

7.2 Recommendations for Deployment
----------------------------------

- Use gpt-4o-mini as primary generator for cost-effectiveness and speed
- Implement partial text transformation for targeted edits
- Add human review for critical communications
- Set up monitoring dashboard for real-time quality tracking

7.3 Future Enhancements
-----------------------

- Add more evaluation metrics (readability score, sentiment analysis)
- Implement A/B testing framework for prompt variations
- Create user feedback collection for continuous improvement
- Expand synthetic data to cover more industry-specific scenarios
- Add support for additional languages


================================================================================
                              END OF REPORT
================================================================================
